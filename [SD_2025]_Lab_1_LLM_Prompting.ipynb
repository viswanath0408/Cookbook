{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cdb004605b6542b19d8d280005612c94": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_511a63ef9b8142538682c14576cea265",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "DataGrid(auto_fit_columns=True, auto_fit_params={'area': 'all', 'padding': 30, 'numCols': None}, corner_renderâ€¦",
                  "application/vnd.jupyter.widget-view+json": {
                    "version_major": 2,
                    "version_minor": 0,
                    "model_id": "788c8b1cad454ea49e89d22bcdbc94f5"
                  }
                },
                "metadata": {}
              }
            ]
          }
        },
        "511a63ef9b8142538682c14576cea265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "788c8b1cad454ea49e89d22bcdbc94f5": {
          "model_module": "ipydatagrid",
          "model_name": "DataGridModel",
          "model_module_version": "^1.4.0",
          "state": {
            "_data": {
              "data": {
                "key": {
                  "dtype": "int32",
                  "shape": [
                    16
                  ],
                  "type": null
                },
                "Provider": [
                  "gemini",
                  "gemini",
                  "gemini",
                  "gemini",
                  "gemini",
                  "gemini",
                  "gemini",
                  "gemini",
                  "openai",
                  "openai",
                  "openai",
                  "openai",
                  "openai",
                  "openai",
                  "openai",
                  "openai"
                ],
                "Model Name": [
                  "gemini/gemini-1.5-flash",
                  "gemini/gemini-1.5-flash-002",
                  "gemini/gemini-1.5-flash-latest",
                  "gemini/gemini-2.0-flash",
                  "gemini/gemini-2.0-flash-001",
                  "gemini/gemini-2.0-flash-lite",
                  "gemini/gemini-2.0-flash-lite-preview-02-05",
                  "gemini/gemini-2.5-flash",
                  "gpt-4.1-mini",
                  "gpt-4.1-mini-2025-04-14",
                  "gpt-4.1-nano",
                  "gpt-4.1-nano-2025-04-14",
                  "gpt-5-mini",
                  "gpt-5-mini-2025-08-07",
                  "gpt-5-nano",
                  "gpt-5-nano-2025-08-07"
                ],
                "Input Cost (per million tokens)": {
                  "dtype": "float64",
                  "shape": [
                    16
                  ],
                  "type": null
                },
                "Output Cost (per million tokens)": {
                  "dtype": "float64",
                  "shape": [
                    16
                  ],
                  "type": null
                },
                "Blended Cost (per million tokens)": {
                  "dtype": "float64",
                  "shape": [
                    16
                  ],
                  "type": null
                },
                "Supports Response Schema": {
                  "value": [
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true
                  ],
                  "type": "raw"
                },
                "Supports Vision": {
                  "value": [
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true
                  ],
                  "type": "raw"
                },
                "Supports Function Calling": {
                  "value": [
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true,
                    true
                  ],
                  "type": "raw"
                },
                "RPM (Requests Per Minute)": {
                  "value": [
                    2000,
                    2000,
                    2000,
                    10000,
                    10000,
                    4000,
                    60000,
                    100000,
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A"
                  ],
                  "type": "raw"
                },
                "TPM (Tokens Per Minute)": {
                  "value": [
                    4000000,
                    4000000,
                    4000000,
                    10000000,
                    10000000,
                    4000000,
                    10000000,
                    8000000,
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A",
                    "N/A"
                  ],
                  "type": "raw"
                },
                "ipydguuid": {
                  "dtype": "int32",
                  "shape": [
                    16
                  ],
                  "type": null
                }
              },
              "schema": {
                "fields": [
                  {
                    "name": "key",
                    "type": "integer"
                  },
                  {
                    "name": "Provider",
                    "type": "string"
                  },
                  {
                    "name": "Model Name",
                    "type": "string"
                  },
                  {
                    "name": "Input Cost (per million tokens)",
                    "type": "number"
                  },
                  {
                    "name": "Output Cost (per million tokens)",
                    "type": "number"
                  },
                  {
                    "name": "Blended Cost (per million tokens)",
                    "type": "number"
                  },
                  {
                    "name": "Supports Response Schema",
                    "type": "boolean"
                  },
                  {
                    "name": "Supports Vision",
                    "type": "boolean"
                  },
                  {
                    "name": "Supports Function Calling",
                    "type": "boolean"
                  },
                  {
                    "name": "RPM (Requests Per Minute)",
                    "type": "string"
                  },
                  {
                    "name": "TPM (Tokens Per Minute)",
                    "type": "string"
                  },
                  {
                    "name": "ipydguuid",
                    "type": "integer"
                  }
                ],
                "primaryKey": [
                  "key",
                  "ipydguuid"
                ],
                "pandas_version": "1.4.0",
                "primaryKeyUuid": "ipydguuid"
              },
              "fields": [
                {
                  "key": "$NaN$"
                },
                {
                  "Provider": "$NaN$"
                },
                {
                  "Model Name": "$NaN$"
                },
                {
                  "Input Cost (per million tokens)": "$NaN$"
                },
                {
                  "Output Cost (per million tokens)": "$NaN$"
                },
                {
                  "Blended Cost (per million tokens)": "$NaN$"
                },
                {
                  "Supports Response Schema": "$NaN$"
                },
                {
                  "Supports Vision": "$NaN$"
                },
                {
                  "Supports Function Calling": "$NaN$"
                },
                {
                  "RPM (Requests Per Minute)": "$NaN$"
                },
                {
                  "TPM (Tokens Per Minute)": "$NaN$"
                },
                {
                  "ipydguuid": "$NaN$"
                }
              ]
            },
            "_dom_classes": [],
            "_model_module": "ipydatagrid",
            "_model_module_version": "^1.4.0",
            "_model_name": "DataGridModel",
            "_transforms": [],
            "_view_count": null,
            "_view_module": "ipydatagrid",
            "_view_module_version": "^1.4.0",
            "_view_name": "DataGridView",
            "_visible_rows": [],
            "auto_fit_columns": true,
            "auto_fit_params": {
              "area": "all",
              "padding": 30,
              "numCols": null
            },
            "base_column_header_size": 20,
            "base_column_size": 64,
            "base_row_header_size": 64,
            "base_row_size": 20,
            "column_widths": {},
            "corner_renderer": null,
            "default_renderer": "IPY_MODEL_e0e6e0d14e634a15af65bad23c9da399",
            "editable": false,
            "grid_style": {},
            "header_renderer": null,
            "header_visibility": "all",
            "horizontal_stripes": false,
            "layout": "IPY_MODEL_c14956b45ab24535963c3597557adccc",
            "renderers": {},
            "selection_mode": "none",
            "selections": [],
            "vertical_stripes": false
          }
        },
        "9742467776e2469a9511a7ed2ed3a672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c273739fe77142999e72d757bcbefd9f",
              "IPY_MODEL_551179acbf8c4ae0a9c507338da21c09",
              "IPY_MODEL_9281cc218518422fb4d9be15839310ef"
            ],
            "layout": "IPY_MODEL_90c2a9cf5c6149769f6751a2c24ab824"
          }
        },
        "c273739fe77142999e72d757bcbefd9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Select a model",
              "gemini/gemini-1.5-flash",
              "gemini/gemini-1.5-flash-002",
              "gemini/gemini-1.5-flash-latest",
              "gemini/gemini-2.0-flash",
              "gemini/gemini-2.0-flash-001",
              "gemini/gemini-2.0-flash-lite",
              "gemini/gemini-2.0-flash-lite-preview-02-05",
              "gemini/gemini-2.5-flash",
              "gpt-4.1-mini",
              "gpt-4.1-mini-2025-04-14",
              "gpt-4.1-nano",
              "gpt-4.1-nano-2025-04-14",
              "gpt-5-mini",
              "gpt-5-mini-2025-08-07",
              "gpt-5-nano",
              "gpt-5-nano-2025-08-07"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Model:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_e9c36f58a1fb4dcebb80db36bd47ff9e",
            "style": "IPY_MODEL_2d4e6d6b5b844dcb8559c2d95acfe0f2"
          }
        },
        "551179acbf8c4ae0a9c507338da21c09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Prompt:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5e81cfd93f5f45fbbb8b95a1038f47d2",
            "placeholder": "Enter your prompt here",
            "rows": null,
            "style": "IPY_MODEL_c98e1ab375074b5ba56d5b9f51b0250c",
            "value": "What is the elevation of Mount Kilimanjaro?"
          }
        },
        "9281cc218518422fb4d9be15839310ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Submit",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_cfa00aab508c458ca740c2c63f9ead29",
            "style": "IPY_MODEL_f7638d871615462daf860cc8e262bdd0",
            "tooltip": ""
          }
        },
        "90c2a9cf5c6149769f6751a2c24ab824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9c36f58a1fb4dcebb80db36bd47ff9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4e6d6b5b844dcb8559c2d95acfe0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e81cfd93f5f45fbbb8b95a1038f47d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "100px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c98e1ab375074b5ba56d5b9f51b0250c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cfa00aab508c458ca740c2c63f9ead29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7638d871615462daf860cc8e262bdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viswanath0408/Cookbook/blob/master/%5BSD_2025%5D_Lab_1_LLM_Prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hands-On Session 1: The Art of Prompting LLMs\n",
        "\n",
        "## Overview\n",
        "\n",
        "Welcome to Part 1(a) of our AI workshop series. In this session, you'll dive into the world of prompting large language models (LLMs), gaining some initial familiarity with effectively communicating with LLMs.\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this lab, you will:\n",
        "1. Craft effective prompts for diverse AI applications\n",
        "2. Understand and apply various prompting techniques\n",
        "3. Gain insights into AI safety through hands-on experience\n",
        "\n",
        "## 0. Using the tech stack\n",
        "\n",
        "* **Google Colab**: We'll use this free, cloud-based platform for easy setup and access to computational resources. While we're focusing on Colab, you can easily replicate this project in your local computing environment. (You're here!)\n",
        "* To run \"cells\" in Colab, either press the \"Play\" button in the upper left of the cell, or type Shift+Enter (hold down the keys together briefly).\n",
        "\n",
        "* **LiteLLM**: This library helps streamline interactions with a wide range of pre-trained LLMs. We'll use it to load and run our LLMs, and interact with the LLM.\n",
        "\n",
        "* **LLM**: We'll start with a handful of OpenAI and Gemini models.\n",
        "\n",
        "____\n",
        "## LAB 1-A: Prompting Fundamentals\n",
        "\n",
        "For this hands-on session, we will explore:\n",
        "\n",
        "- **Prompt Anatomy**: Explore system instructions, user inputs, and AI responses.\n",
        "- **Token Mechanics**: Understand what tokens are, how they are used in LLMs, and how to estimate both input and output costs of token use.\n",
        "- **Conversation Dynamics**: Compare single-turn queries and multi-turn dialogues.\n",
        "\n",
        "To begin, we need to install litellm in order to have a simple, unified interface to communicate with an LLM"
      ],
      "metadata": {
        "id": "kufoYFUT4eT4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCAD9xi04YWW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "cellView": "form",
        "outputId": "f93dd9a9-7c4e-4469-9ae6-4d58c0648767"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CapturingDisplayPublisher' object has no attribute 'register_hook'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-573335158.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipydatagrid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataGrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_custom_widget_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# File ID from Google Drive link\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_widgets.py\u001b[0m in \u001b[0;36menable_custom_widget_manager\u001b[0;34m(version)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;34m'Unknown widgets version: {version}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0;31m   _install_custom_widget_manager(\n\u001b[0m\u001b[1;32m     40\u001b[0m       \u001b[0;34m'https://ssl.gstatic.com/colaboratory-static/widgets/colab-cdn-widget-manager/{version_hash}/manager.min.js'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion_hash\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mversion_hash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_widgets.py\u001b[0m in \u001b[0;36m_install_custom_widget_manager\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     59\u001b[0m   \u001b[0;32mglobal\u001b[0m \u001b[0m_installed_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_installed_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_widget_display_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_installed_url\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay_pub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_widget_display_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CapturingDisplayPublisher' object has no attribute 'register_hook'"
          ]
        }
      ],
      "source": [
        "#@title Installing Necessary Packages and Imports\n",
        "%%capture\n",
        "%pip install litellm ipydatagrid\n",
        "\n",
        "import gdown\n",
        "import os\n",
        "import litellm\n",
        "litellm.drop_params = True\n",
        "\n",
        "from litellm import completion\n",
        "from litellm import model_cost\n",
        "import pandas as pd\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown, clear_output\n",
        "from collections import defaultdict\n",
        "from ipydatagrid import DataGrid\n",
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()\n",
        "\n",
        "# File ID from Google Drive link\n",
        "file_id = '1fHsWJRXEJVL3PPYhNuCYklIhNHAJXnYE'\n",
        "destination = '/content/env_vars.txt'\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id}', destination, quiet=False)\n",
        "\n",
        "# Read the file and set environment variables\n",
        "with open(destination, 'r') as file:\n",
        "    exec(file.read())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Extract relevant available models given a few constraints\n",
        "\n",
        "def filter_models_by_cost_and_provider(acceptable_providers: list, mode: str, max_input_cost_per_token_million: float, max_output_cost_per_token_million: float, structured_responses: bool) -> dict:\n",
        "    \"\"\"\n",
        "    Filters models from the global model_cost dictionary based on acceptable providers and max token costs.\n",
        "\n",
        "    Args:\n",
        "        acceptable_providers: A list of strings representing the acceptable providers.\n",
        "        mode: The mode of the model. (chat, embedding, etc. Based on LiteLLM)\n",
        "        max_input_cost_per_token_million: The maximum acceptable input cost per token (in dollars per million tokens).\n",
        "        max_output_cost_per_token_million: The maximum acceptable output cost per token (in dollars per million tokens).\n",
        "        structured_responses: A boolean indicating whether we want the model to support structured responses\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are provider names and values are lists of model information\n",
        "        dictionaries that meet the specified criteria, including their blended token cost.\n",
        "    \"\"\"\n",
        "    filtered_models = defaultdict(list)\n",
        "    for model_name, model_info in model_cost.items():\n",
        "        provider = model_info.get('litellm_provider')\n",
        "        input_cost_per_token = model_info.get('input_cost_per_token', 0)\n",
        "        output_cost_per_token = model_info.get('output_cost_per_token', 0)\n",
        "        model_mode = model_info.get('mode')\n",
        "        supports_response_schema = model_info.get('supports_response_schema', False)\n",
        "        supported_output_modalities = model_info.get('supported_output_modalities', [])\n",
        "\n",
        "        if structured_responses and not supports_response_schema:\n",
        "            continue\n",
        "\n",
        "        # Convert cost per token to cost per million tokens for comparison\n",
        "        input_cost_per_token_million = round(input_cost_per_token * 1_000_000, 4)\n",
        "        output_cost_per_token_million = round(output_cost_per_token * 1_000_000, 4)\n",
        "\n",
        "        # Calculate blended cost (simple average for now)\n",
        "        blended_cost_per_token_million = round((input_cost_per_token_million + output_cost_per_token_million) / 2, 4)\n",
        "        if blended_cost_per_token_million == 0 or model_mode != mode:\n",
        "            continue\n",
        "        if provider in acceptable_providers and \\\n",
        "            input_cost_per_token_million <= max_input_cost_per_token_million and \\\n",
        "            output_cost_per_token_million <= max_output_cost_per_token_million:\n",
        "            # Include all model_info and the blended cost\n",
        "            model_info_with_cost = model_info.copy()\n",
        "            model_info_with_cost['blended_cost_per_token_million'] = blended_cost_per_token_million\n",
        "            # Additional filtering for openai models based on supported_output_modalities\n",
        "            if provider == 'openai':\n",
        "                if 'text' not in supported_output_modalities:\n",
        "                  continue\n",
        "            if provider == 'gemini':\n",
        "              ### Skipping the below two models due to issues\n",
        "              if model_name in [\"gemini/gemini-1.5-flash-001\", \"gemini/gemini-2.0-flash-preview-image-generation\"]:\n",
        "                continue\n",
        "              ### Small filter on requests per minute just to reduce returned list a bit.\n",
        "              if model_info['rpm'] < 100:\n",
        "                continue\n",
        "            filtered_models[provider].append({model_name: model_info_with_cost})\n",
        "\n",
        "\n",
        "    return filtered_models\n",
        "\n",
        "available_models = filter_models_by_cost_and_provider(acceptable_providers=['openai', 'gemini'],\n",
        "                                                      mode='chat',\n",
        "                                                      max_input_cost_per_token_million=3,\n",
        "                                                      max_output_cost_per_token_million=3,\n",
        "                                                      structured_responses=True)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "DChCO5_nZuLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a simple LLM\n",
        "\n",
        "---\n",
        "Now let's try making a very simple LLM, technically a text-completion model."
      ],
      "metadata": {
        "id": "vHJ1HKm_EOf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model options and sort them\n",
        "sorted_models_list = []\n",
        "for provider in ['gemini', 'openai']:\n",
        "    if provider in available_models:\n",
        "        provider_models = sorted([list(model.keys())[0] for model in available_models[provider]])\n",
        "        sorted_models_list.extend(provider_models)\n",
        "\n",
        "all_models_list = sorted_models_list\n",
        "\n",
        "\n",
        "# Create the dropdown widget\n",
        "model_dropdown = widgets.Dropdown(\n",
        "    options=['Select a model'] + all_models_list,\n",
        "    value='Select a model',\n",
        "    description='Model:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "# Create a text input widget for the prompt\n",
        "input_text = widgets.Textarea(\n",
        "    value='What is the elevation of Mount Kilimanjaro?',\n",
        "    placeholder='Enter your prompt here',\n",
        "    description='Prompt:',\n",
        "    layout=widgets.Layout(width='50%', height='100px') ,\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "# Create a button\n",
        "submit_button = widgets.Button(description=\"Submit\")\n",
        "\n",
        "\n",
        "# Output widget to display the table and responses\n",
        "output_area = widgets.Output()\n",
        "\n",
        "# Display the output area for the table and responses\n",
        "display(output_area)\n",
        "\n",
        "# Function to display model comparison table\n",
        "def display_model_comparison_table(models_dict):\n",
        "    data = []\n",
        "    # Collect data and sort it\n",
        "    for provider, models_info in models_dict.items():\n",
        "        for model_data in models_info:\n",
        "            for model_name, info in model_data.items():\n",
        "                data.append({\n",
        "                    'Provider': provider,\n",
        "                    'Model Name': model_name,\n",
        "                    'Input Cost (per million tokens)': round(info.get('input_cost_per_token', 0) * 1_000_000, 4),\n",
        "                    'Output Cost (per million tokens)': round(info.get('output_cost_per_token', 0) * 1_000_000, 4),\n",
        "                    'Blended Cost (per million tokens)': info.get('blended_cost_per_token_million', 0),\n",
        "                    'Supports Response Schema': info.get('supports_response_schema', False),\n",
        "                    'Supports Vision': info.get('supports_vision', False),\n",
        "                    'Supports Function Calling': info.get('supports_function_calling', False),\n",
        "                    'RPM (Requests Per Minute)': info.get('rpm', 'N/A'),  # Add RPM\n",
        "                    'TPM (Tokens Per Minute)': info.get('tpm', 'N/A')    # Add TPM\n",
        "                })\n",
        "\n",
        "    if data:\n",
        "        df = pd.DataFrame(data)\n",
        "        df_sorted = df.sort_values(by=['Provider', 'Model Name']).reset_index(drop=True)\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            grid = DataGrid(df_sorted)\n",
        "            grid.auto_fit_columns=True\n",
        "            display(grid)\n",
        "        return df_sorted\n",
        "    else:\n",
        "        with output_area:\n",
        "            clear_output(wait=True)\n",
        "            print(\"No models available based on the current filter.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Display the initial table\n",
        "available_models_df = display_model_comparison_table(available_models)\n",
        "\n",
        "# Arrange dropdown, input text and button vertically below the table\n",
        "input_widgets = widgets.VBox([model_dropdown, input_text, submit_button])\n",
        "display(input_widgets)\n",
        "\n",
        "\n",
        "# Function to handle button click\n",
        "def on_submit_button_clicked(b):\n",
        "    selected_model = model_dropdown.value\n",
        "    user_prompt = input_text.value\n",
        "\n",
        "    if selected_model == \"Select a model\":\n",
        "        with output_area:\n",
        "            print(\"Please select a model.\")\n",
        "        return\n",
        "    if not user_prompt:\n",
        "        with output_area:\n",
        "            print(\"Please enter a prompt.\")\n",
        "        return\n",
        "\n",
        "    messages = [{\"content\": user_prompt, \"role\": \"user\"}]\n",
        "\n",
        "    with output_area:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Selected model: {selected_model}\")\n",
        "        print(\"Sending API call...\")\n",
        "        try:\n",
        "            response = completion(model=selected_model, messages=messages)\n",
        "            display(Markdown(response.choices[0].message.content))\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Register the callback function\n",
        "submit_button.on_click(on_submit_button_clicked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "cdb004605b6542b19d8d280005612c94",
            "511a63ef9b8142538682c14576cea265",
            "788c8b1cad454ea49e89d22bcdbc94f5",
            "9742467776e2469a9511a7ed2ed3a672",
            "c273739fe77142999e72d757bcbefd9f",
            "551179acbf8c4ae0a9c507338da21c09",
            "9281cc218518422fb4d9be15839310ef",
            "90c2a9cf5c6149769f6751a2c24ab824",
            "e9c36f58a1fb4dcebb80db36bd47ff9e",
            "2d4e6d6b5b844dcb8559c2d95acfe0f2",
            "5e81cfd93f5f45fbbb8b95a1038f47d2",
            "c98e1ab375074b5ba56d5b9f51b0250c",
            "cfa00aab508c458ca740c2c63f9ead29",
            "f7638d871615462daf860cc8e262bdd0"
          ]
        },
        "id": "V4OR89eiKG2Q",
        "outputId": "af66f3f6-ca3a-43ca-a94d-73320cd1b8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb004605b6542b19d8d280005612c94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Dropdown(description='Model:', options=('Select a model', 'gemini/gemini-1.5-flash', 'gemini/geâ€¦"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9742467776e2469a9511a7ed2ed3a672"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For those who want to peer a little bit deeper into the response object, take a look at the output from the below cell. Notice a few things:\n",
        "1. Although we selected gpt-5-nano, the model that was actually used was gpt-5-nano-2025-08-07. **This indicates that the gpt-5-nano model is really just a pointer to most likely the latest version of the model**.\n",
        "2. Gpt-5-nano also used a few hundred reasoning_tokens for internal \"thinking\" before producing the final answer. This shows the model's chain-of-thought reasoning process that happens behind the scenes. This is a controlled parameter, but not available for older models.\n",
        "3. If you had any tools used, like a web search, or a document lookup, it would appear here under your tool_calls field.\n",
        "\n",
        "\n",
        "```python\n",
        "user_prompt = \"What is the elevation of Mount Kilimanjaro?\"\n",
        "\n",
        "messages = [{\"content\": user_prompt, \"role\": \"user\"}]\n",
        "\n",
        "response = completion(model='gpt-5-nano', messages=messages)\n",
        "\n",
        "print(response.model_dump_json(indent=2))\n",
        "```\n",
        "```json\n",
        "{\n",
        "  \"id\": \"chatcmpl-C6q45tUmRdFjIARARVGyuhlLKTKdB\",\n",
        "  \"created\": 1755746141,\n",
        "  \"model\": \"gpt-5-nano-2025-08-07\",\n",
        "  \"object\": \"chat.completion\",\n",
        "  \"system_fingerprint\": null,\n",
        "  \"choices\": [\n",
        "    {\n",
        "      \"finish_reason\": \"stop\",\n",
        "      \"index\": 0,\n",
        "      \"message\": {\n",
        "        \"content\": \"Approximately 5,895 meters (19,341 feet) above sea level. The summit is Uhuru Peak.\",\n",
        "        \"role\": \"assistant\",\n",
        "        \"tool_calls\": null,\n",
        "        \"function_call\": null,\n",
        "        \"annotations\": []\n",
        "      },\n",
        "      \"provider_specific_fields\": {}\n",
        "    }\n",
        "  ],\n",
        "  \"usage\": {\n",
        "    \"completion_tokens\": 352,\n",
        "    \"prompt_tokens\": 17,\n",
        "    \"total_tokens\": 369,\n",
        "    \"completion_tokens_details\": {\n",
        "      \"accepted_prediction_tokens\": 0,\n",
        "      \"audio_tokens\": 0,\n",
        "      \"reasoning_tokens\": 320,\n",
        "      \"rejected_prediction_tokens\": 0,\n",
        "      \"text_tokens\": null\n",
        "    },\n",
        "    \"prompt_tokens_details\": {\n",
        "      \"audio_tokens\": 0,\n",
        "      \"cached_tokens\": 0,\n",
        "      \"text_tokens\": null,\n",
        "      \"image_tokens\": null\n",
        "    }\n",
        "  },\n",
        "  \"service_tier\": \"default\"\n",
        "}\n",
        "```\n"
      ],
      "metadata": {
        "id": "PmF33tbroGo1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations**, you just created & programmatically interacted with your first LLM. ðŸŽ‰\n",
        "\n"
      ],
      "metadata": {
        "id": "JVi1lIDLJDWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1\n",
        "\n",
        "Enter a new \"prompt_factoid\" below to test the LLM's responses.\n",
        "\n",
        "Try:\n",
        "\n",
        "*   Changing the question to a different 'factoid';\n",
        "*   Asking a more open-ended question;\n",
        "*   Asking a more business-specific question.\n",
        "\n",
        "\n",
        "*You can also go back to the previous code above and change the LLM model! Notice how the same prompt can have significantly different responses.*\n"
      ],
      "metadata": {
        "id": "rxXxpN4iKcu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_model = \"gemini/gemini-2.0-flash\" #@param ['gemini/gemini-1.5-flash', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-2.0-flash', 'gemini/gemini-2.0-flash-001', 'gemini/gemini-2.0-flash-lite', 'gemini/gemini-2.0-flash-lite-preview-02-05', 'gemini/gemini-2.5-flash', 'gpt-4.1-mini', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano', 'gpt-4.1-nano-2025-04-14', 'gpt-5-mini', 'gpt-5-mini-2025-08-07', 'gpt-5-nano', 'gpt-5-nano-2025-08-07']\n",
        "\n",
        "user_prompt = \"Tell me a funny joke\" # @param {\"type\":\"string\",\"placeholder\":\"Tell me a joke\"}\n",
        "\n",
        "messages = [{\"content\": user_prompt, \"role\": \"user\"}]\n",
        "\n",
        "print(\"Sending API call...\")\n",
        "response = completion(model=llm_model,\n",
        "                      messages=messages,\n",
        "                      temperature=0.3)\n",
        "display(Markdown(response.choices[0].message.content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "yNQ07_0gHKDP",
        "outputId": "face71dc-a5bb-454a-a2c7-f148b991685a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sending API call...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why don't scientists trust atoms?\n\nBecause they make up everything!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAUSE** here. Did you do the task above? What did you learn?\n",
        "\n",
        "![pause pillow.jpeg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wCEAAkGBxITEhUQEBMQEBUVERIVFRUWEBAPFRUVFREWFhUVFhcYHSggGBonGxUVITIiJSkrLi4uFx8zODMuNygtLisBCgoKBQUFDgUFDisZExkrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrKysrK//AABEIAOEA4QMBIgACEQEDEQH/xAAcAAEAAQUBAQAAAAAAAAAAAAAABwECAwYIBQT/xABNEAABAwICBgUEDgcFCQAAAAABAAIDBBEFEgYHEyExcUFRYYGRFCKhsyMyNUJScnN0gpKisbLBFTM0U2KT0RckQ2OECCU2o8LD0vDx/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/EABQRAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhEDEQA/AJxREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERARFGek+tfyWrlpG0m1ERDXPM+yJcWNfuaGO3ecOlBJiKHKfXRLm9ko4yy/vJ3BwHeyzj4LbsF1o4bPufI6kd8GdojH8wEs+1fsQbqiw01VHIM0b2SN62ua8eIWZAREQEREBERAREQEREBERAREQEREBERARfDiuMU9M3PUzRQt63va2/YBxJ7AtLq9cGHtdlYyrnHw2RRtb4Sva70IJCRa3o7pxRVjxFBI4SEE7N8b43WAubEjKe4lbIgIiIC5/wBdVFs8S2gFhPBG+/W5l43ehrPFdAKLNfmF5qWGrHGGbI74kwA/G2PxQQ6wrJlWCFy+lqClPmjOaJz4nfCY90bvFpBWw0GnGJw2DKuV4HRIGT+l4LvSvDAVbINzh1sYk0+cKR46jC9vpD17EGuiTdtKFp6y2qI8AY/zUaZVQsQTRQ64KBw9lZVQHpvEJB3GMkkcwF9jNa+EnjPK3nSVf5MUFGNWmJB0hh+meHzC8dXTm3EOkEThza+x9C+hmk9CSWispCRxHlMNxz85cymAKhpx1IOom43SnhUUx5TxH81k/SkH76H+az+q5XNKOoeCClb1DwQdQT6Q0bBd9VSsFid9RENw4neeCuix6kcMzammcOsTxEfeuXRTDq9CvNOOoIOk6rTLDo9z62kB6tvG4+AJK8mo1pYU05fKHP8AiU9S8eOSxUACALIIggnf+1fCv30o/wBLU/8AgrTrZwq9ttKf9LU/mxQUYwrHQgoJ8frUwoMzid5/gFPUB57i0DvvZeJPrqpd+ypatxsbZ9hG0ntIe4gd3cob8k61nbCEG9YlrcrpLiGOCmF9xAM7wObvN+yvAqtN8TkFn1kwFreZkh9LGgrxsiZUGKQFzi97nSOPFz3Oe483O3lXBqvsrXFBvGpqjz4htOiGCR30nlrG+gv8FOqi3URQ2iqak+/lZEOUbMx9MvoUpICIiAvL0nwltXST0rv8WJ7Qep1rsdzDg09y9REHIdKTYXFj0jpB6ivtYvo0modhX1cJ97VSkfFe7aM+y9q+aIoMwVwCtasgQLJZFVBSyWVVRALf/ee9Usp0o9FaWtw+k27LPFHAGyts2RvsTenpHYbhRzpRoBVUl3tHlEI9+xpzNH8bOI5i45INRshCqqoLQ1Z46CVwzNilcDwIje4HkQFiU/atPc2n+LJ656CCP0ZP+5n/AJMn9E/Rk37mf+TJ/RdPIg5alhc05Xtc09Tmlp8CrA1blrX90pPk4fwBaggpZUsrlRBSyorlQoLSsExWZywiEyObE3cZHtjHN7g0ekoOidWmH7DDKVtrF8QmdfjmmJlIPLOB3LZ1jp4QxjWN4Na1o5NFh9yyICIiAiIggbXjhuzr46ge1qIBf48Lsrifouj8Fo8PBSr/ALQNMclHNfcJZY7WPF7A8G/D/CPiophKD6Wq9WNV6CqIiAqKqog6O0P/AGCk+aU/qmr2F4+h/wCwUnzSn9U1ewg0zSnV3TVV5Iv7tMd+Zo8xx/jZw7xY81EmkOjVTRuy1EZDSbNkb50buTug9hsV0csdRAyRpZI1r2uFnNc0OaR1EHig5eU/atPc2n+LJ6561nSjVY115KB2zPHYvJLD8Rx3t5G45LbdAqOSGggimYY3tEgc08Qds8/cg2BERBBOtf3Sk+Th/AFqC2/Wv7pSfJw/gC1BAREQFQqqoUGN69XQWk2uJUjLXtO2Q9giBkv9leTIVImorD2uqKmpPGKOONvZtXOLj/ywO8oJnREQEREBERBHmvWMHDMx4sqoC3sJJYfQ4qEKbgp212QOdhUpaL5JYHu7GiVoJ7r35XUC0b7oPuarla1XBBVERAVFVUQdHaH/ALBSfNKf1TV7C8fQ/wDYKT5pT+qavXJtvO5BVfPXVscLDLM9kbBxc4ho/wDvYtL0p1mQQXjpbVMg3ZgfYmntcPb8m+KibGsbqKt+0qZHSH3o4MZ2NaNw+/rug37SnWmTeLD25Rw2727+bGHhzd4LddXtQ+TD4JJHOke4SFznOLnE7Z/Elc+KftWnubT/ABZPXPQbOiIggnWv7pSfJw/gC1BbfrX90pPk4fwBaggIiICoVVUKDDIpX1CQO2VXKQcrpYmA9BLGOLrfzAonqHWF1P2qWJrcKpsvvhK49rnTvJv93cg29ERAREQEREHl6USRto6l0wBjFNOXgi4LRE7MLdO5cp4UDYX6guiNc9YY8KmaDYyvhi7nStLx3sa8d6gGlZZB9zVcrGq8IKoiICoqogmqm00paKgpWvdtJfJKe0TCC79U32x4MHPwKjjSfTSqrbte7ZRdELCQ0j+M8X9+7sWtqqCiqiICn7Vof920/wAWT1z1AKpZB1LdLrlqyWQbhrX90pPk4fwBagqKqAiIgKhRUKD56reLKddS0t8Kjbe5ZNUNI6rzueB4PB71BcgUu6h6kbGqh6Wzsl7pIgz/ALRQSiiIgIiICIiCMtfs4FFBHuu6saQOmzYZbn0jxUMwLfNe+JNkrYadt708Li49F5i0gDtAYD9ILRIQg+lqvCxtV4QXKrWkkAAkk2AAuSTwACtXp6M/tlN86g9a1B8rqCYOawxTBzr5WmJ4c63HKLXPcrqzC54gDNBPCDwMkMkYPYC4BTZrD0kFCyORkbJJ352Rl4JDGeaZDu38RHuuOjqXx6CaWfpJs1NVxxFwZcgA5JIybG7STYg26ekIIUX30+CVT25o6aqkb0ObTzOB5ENsVsFLFS4fikratr5Y4XO2bQ1r97sroi4OIBs13iAtlxPTbEppr4bTTbGzcuele4vNt5c6+UC+7cejigjCop3sdlkY+N3wXscx3g4XVzaOUtziOUssTmEby2w4nNa1txUy60qUPw3bSsa2VhhcOBLHPc1r2g9I3kdwWPAP+H3fNKv8UqCGoo3OIa0OcTwDQXE8gOKunp3sNpGPjJFwHMcwkddiF7+rn3Spfjv9RItu1nRRvxOhZMQI3CNr7mwymos656BbpQRzSYVUSjNDBUSjrZDLIPFoIWGppZIzlljkid8F7HRu8HAFTzpg7EI44xhccRDbh7crMwaAMoY1xDbce3hZaZpPpUJaI02JUdRHUFrsj9kGRiUXyPYXG9uF7X6UEaAXNhvJNgOJJ6gvumwWqa3aPpqpjLXzOp5mtA6yS2wCk3VhgjIaR2IujM0rmyGNoGZwYy4ysHwnFp39oC+nA9KMVfUsbU0L2QveGkthlBjB3BxcTYgbr7huughpVW9a2cAjp6hk0LQxk4eS0CwEjCMxA6LhwNusFaKgoqFVVpQY3BSBqNkIrahgO40oJHa2Vob6HO8VH71sWq/EDBicJvZsuaF3J4u37bWIOikREBERARF4OneLeS4fU1ANnNhcIz/mP8yP7bmoOdtLcRFTiFVUDg+oeG9rY7RMPe1gPevljXx0sdgB1BfbGEGZqvCsarroLl6WjX7ZTfOoPWtXl3Xp6Mn++U3zqD1rUEta19HJqqKKSnaZHwufdgIzFkgbctvxILG7u0rz9U2jNRA+WpqI3Q3j2bGu3ON3BznEdA80Df2r6NbOLz0rqSWnkMbrzg2sQRaPc4HcQtBxXT6vnjMT5QxrhZwjYIy4dRPG3KyDdtFDT1eMVtSQ2TIGbG9iPNDYjIOv2gsf41h0wxfGfK309LHMyO4EZjgDg4Fo84ykEDeT0iyjLDMSlp5BNA90T28CLcDxBB3EdhWx1OsrEXty7VjLixcyJjXeJvbuQSNrHjc3CHNeczmimDncbuEjA4+Kx6FQ7fBNgwjM6Gqi5Oc+S1/rA96iur0wrZIPJZZi+LK1tnMYXENIIu+2YncN97rHgGlNVR38mkytcbuY5oewnrseB7RZBt+r3Q+sjro56iF0LIs5JcWb3FjmBrbE33uvfhuX2a1KNs2IUkD3iISRhmctzBpdK4C4uL77Dj0rUMQ0+r5Xse6YN2bg9rWsa1mYcCRvzd915WOaQT1bmvqX7RzW5QcjGWF7280DpKCRcVpMXw3ZimnmrYiLfqNrkIO5pbdzgLcCCB0LaxO6pwx78ThbATFIXtILbBoOV4B3sduBA4gqKMM1hYhC0MbMJGgWG0Y2QgfG9se8r5Me0vrKsZJ5bsuDs2tEbCRwJA9t33QShq2xAzYWYIXhk8TZIwSAcjnZnRPIPEbx9UrxKKfSJ8ohcXxedZ0joafZtHS7MG2dyHFRzheKzU8glp5HRPG646R1EHc4dhWyS6zMRLcu1jbu9s2Fgd6bj0IM2so1LJI6eqq2Vbmtc+zYWQ7PNYC9uJIb6O1aYrqioc9xkkc57nG7nOJcSeskrHdBcVaUuqILHq+glyTwPG7LUQu+rK0/krSvmnJ3uHEbxzG8IOtEWGiqBJGyRvB7GvHJzQR96zICIiAop1+Yrlgp6MHfLKZXj+CIWAP03tP0FKy5w1pYg6fFJ7+1hyQsHYxt3fbc/wBCDV4mr6GrG1HTAcSAgzgquZfXh+B1k++CmqJQeDhE4NP03Wb6VsFBqwxST20cNOP82Zv3R50GqZkEljcGxHA3sQpBo9TlWT7NU00Yv7xss5tfqcGb7eHavcGpim6aqrvygA8Mn5oIkmq3O9u977cMznOtyusJlU00mp2hb+slqpfpxxgfVZf0r1INV2FNFjTuf2uqKgn8dkEAmZYjVt6x4rpJugeFgW8hpTzia495O8r3IKKJjQxkcbGjcGtY1oHIAIOT21jTwc094WQSrqt9DEfbRxnmxp/JeVJobhziXOoqNxcbm9PEbnr4IOZZagDiQL7h2nsVGzb7HcergV1RQ4JSw74aeniPWyGNh8QFlrsLgmFp4YZh1PjZJ94QcstlVdsukKrQjDZBldRUw3Wu2JsThycyxHivLk1V4UTfyd45VNUP+tBAYlVwepiqdTFIS4xVFVHcktadlI1nUN7czgO11+1eV/YrLe3l8duk+ROv4bb80EZ5lXMt1xTVLiER9hdBVt7HeTv72POUfXK1+u0RxGJ2WSjqL2vdjNu360eYX7EHl5lS6+d0tiWu80tNi0+a4HqIO8FXCRBmJWJ4TOhKDoXVXXbXC6YnjG10J6d0L3Rt+y1p71tiinUViwyT0Tjva8Ts7WuAY8DkWtP01KyAiIgLStJdWdFVyOn9lglebufG4EONrXcxwI8LLdUQRpS6mqQOvLUVUg+CDFGDzIaT4ELcMG0RoaW2wpoWnd55btJDbre+7vSvbRAREQEREBERAREQEREBERAREQEREBERBimpmPBa9jHh24hzQ4HmDxWuYhq9wyYEOpIoyd+aIGnde/G8dltCIIjxzU1xdQ1Bvv8AY5wCOQkYLjvaea1Iat8Vz7Pybp9vtoMnO+a9u666JRBGervV5U0VT5VPLB+qezJHneTmLTvcQ0CxaOgqTERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQEREBERAREQf/Z)"
      ],
      "metadata": {
        "id": "VJ_v9jH6Kxwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a Chat LLM\n",
        "---\n",
        "Now let's try creating a slightly-more sophisticated model, a Chat LLM.\n",
        "\n",
        "What makes a chat model different from another LLM? A Chat LLM can engage in dialogue, going back & forth with the user, unlike a text-completion LLM which responds (once) to a (single) prompt. Therefore, we'll have to create a slightly more complex set of prompts to interact with the model, including System Messages and User (or Human) Messages."
      ],
      "metadata": {
        "id": "hp7vOKjZLMtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Anatomy\n",
        "\n",
        "We usually think of a \"prompt\" as a simple piece of text that we send to a model, as we have been doing thus far. But prompts can include more than that. For example, they might include **system instructions** (like, \"Act as an expert in Python programming\") or different **parameters** like temperature to control randomness.\n",
        "\n",
        "There are three types of messages with an LLM:\n",
        "\n",
        "* **system messages** describe the behavior of the AI assistant. A useful system message for data science use cases is \"You are a helpful assistant who understands data science.\"\n",
        "* **user messages** describe what you want the AI assistant to say. We'll cover examples of user messages throughout this tutorial\n",
        "* **assistant messages** describe previous responses in the conversation. We'll cover how to have an interactive conversation later in this lab.\n",
        "\n",
        "The first message should be a system message. Additional messages should alternate between the user and the assistant.\n",
        "\n",
        "Let's create a chat LLM, then be a little more specific about our prompts to it."
      ],
      "metadata": {
        "id": "XFp9G5oeLOU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to select a new LLM from our list.\n",
        "llm_model = \"gemini/gemini-2.0-flash\" #@param ['gemini/gemini-1.5-flash', 'gemini/gemini-1.5-flash-002', 'gemini/gemini-1.5-flash-latest', 'gemini/gemini-2.0-flash', 'gemini/gemini-2.0-flash-001', 'gemini/gemini-2.0-flash-lite', 'gemini/gemini-2.0-flash-lite-preview-02-05', 'gemini/gemini-2.5-flash', 'gpt-4.1-mini', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano', 'gpt-4.1-nano-2025-04-14', 'gpt-5-mini', 'gpt-5-mini-2025-08-07', 'gpt-5-nano', 'gpt-5-nano-2025-08-07']\n",
        "\n",
        "# Define the system message\n",
        "SYSTEM_MESSAGE = 'You are a helpful assistant who understands the business of interior decorating.'\n",
        "\n",
        "# Define the user message\n",
        "USER_MESSAGE = 'What color paint should I use for a living room in a house on the beach in San Diego?'\n",
        "\n",
        "messages_chat = [\n",
        "    {\"content\": SYSTEM_MESSAGE, \"role\": \"system\"},\n",
        "    {\"content\": USER_MESSAGE, \"role\": \"user\"}\n",
        "]\n",
        "\n",
        "### Note: We can add streaming in order to help with seeing the initial outputs sooner.\n",
        "response = completion(model=llm_model, messages=messages_chat, temperature=1, stream=True)\n",
        "res_so_far = \"\"\n",
        "for part in response:\n",
        "  if part.choices[0].delta.content:\n",
        "    res_so_far += part.choices[0].delta.content\n",
        "  print(res_so_far)\n",
        "  clear_output(wait=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "GjmYB2iGGI33",
        "outputId": "047a94ef-36d1-4c92-fd5c-098e308ffefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-597373779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages_chat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mres_so_far\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mres_so_far\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/litellm_core_utils/streaming_handler.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1532\u001b[0m                     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m                     \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompletion_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m                     print_verbose(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2057\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2058\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2059\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse_iterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2060\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"accumulated_json\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulated_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_lines\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLineDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_text\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbyte_content\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m                 \u001b[0mtext_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    895\u001b[0m             \u001b[0mchunker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mByteChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mraw_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m                     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36miter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mraw_stream_bytes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_bytes_downloaded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchunker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stream_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_httpcore_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mShieldCancellation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"receive_response_body\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1230\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(Markdown(res_so_far))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "hBzd95abOlbb",
        "outputId": "93b24aef-817f-4783-e062-c533c347aed2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, a beach house in San Diego! That sounds lovely. When choosing a paint color for your living room, several factors come into play to create that perfect coastal vibe. Here are some popular paint color directions, along with some specific suggestions, and things to consider:\n\n**Overall Goals & Considerations:**\n\n*   **Light and Airy:** Beach houses generally benefit from feeling light and airy, maximizing the natural light and reflecting the outdoors.\n*   **Coastal Vibe:** Think about the specific type of coastal vibe you want. Is it a bright, breezy, classic beach look? A more sophisticated, muted coastal style? Or a warmer, more bohemian beach feel?\n*   **Existing Elements:** Consider existing furniture, flooring, and architectural details (like trim). You want the paint to complement, not clash. Are there any prominent colors in your furniture or decor that you want to play off of?\n*   **Natural Light:** San Diego has great sunlight, but consider the *direction* the windows face. A north-facing room might need warmer tones to compensate for cooler light, while a south-facing room can handle cooler colors.\n*   **Size of the Room:** Lighter colors will make a smaller room feel bigger.\n\n**Color Palette Options & Specific Paint Color Suggestions:**\n\nHere are some popular paint color directions for beach living rooms, along with specific color suggestions from popular brands (but always test samples in your space!):\n\n**1. Classic Coastal (Light & Breezy):**\n\n*   **Goal:** Creates a clean, fresh, and inviting space. This is a very popular and versatile option.\n*   **Colors:** Primarily whites, off-whites, and very pale blues or greens. Think of sand, seafoam, and clouds.\n*   **Suggestions:**\n    *   **Benjamin Moore \"White Dove\" (OC-17):** A classic, soft white that works well in almost any light.\n    *   **Sherwin-Williams \"Sea Salt\" (SW 6204):** A very popular, soft, muted green-blue that evokes the ocean.\n    *   **Benjamin Moore \"Cloud White\" (OC-130):** A warmer white that is soft and inviting.\n    *"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Congratulations**, you just created & programmatically interacted with your first chat LLM. ðŸŽ‰"
      ],
      "metadata": {
        "id": "oyO8Su2HPVQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2\n",
        "\n",
        "Try setting a different `SYSTEM_MESSAGE` variable in the below cell and see how it impacts the response.\n",
        "\n",
        "Try:\n",
        "\n",
        "*   Changing the role of the system;\n",
        "*   Setting a \"stylized\" response for the system (e.g.: 'in the style of Gandalf from The Hobbit');\n",
        "*   Etc.\n"
      ],
      "metadata": {
        "id": "xoW2XqsZPW1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system message\n",
        "SYSTEM_MESSAGE = \"Enter a new system message! Something like 'You are a lazy hourly worker at a paint store' or Respond in the style of Gandalf from The Hobbit\" #@param {\"type\": \"string\"}\n",
        "\n",
        "# Define the user message\n",
        "USER_MESSAGE = 'What color paint should I use for a living room in a house on the beach in San Diego?'\n",
        "\n",
        "messages_chat = [\n",
        "    {\"content\": SYSTEM_MESSAGE, \"role\": \"system\"},\n",
        "    {\"content\": USER_MESSAGE, \"role\": \"user\"}\n",
        "]\n",
        "\n",
        "### Note: We can add streaming in order to help with seeing the initial outputs sooner.\n",
        "response = completion(model=llm_model, messages=messages_chat, temperature=1)\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Cm-HCpYdOtKK",
        "outputId": "0de327d7-2020-4db9-ba00-53e65b9a4f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Option 1: The Laid-Back Beach Bum (lazy hourly worker at a paint store)**\n\nUgh, another customer... Look, pal, just grab some white. Everyone wants white for a beach house. It's easy, it's bright, it goes with everything. We got like, five different whites here. Just pick one, I don't care. Maybe \"Seashell White\" if you want to sound fancy. Just get out of here so I can get back to my phone.\n\n**Option 2: The Gandalf the Grey (from The Hobbit)**\n\nAh, a question of color, a question of light and shadow! For a living room by the shores of San Diego, where the sun dances upon the waves like diamonds, I would advise thee to consider the spirit of the sea itself.\n\nA soft, gentle grey, like the mist that rises from the ocean at dawn, could bring a sense of peace and tranquility. Or perhaps a warm, sandy beige, reminiscent of the dunes where the sea oats sway in the wind. These colours would capture the essence of the coast and reflect the natural light in a harmonious way.\n\nBut be warned, choosing the right colour is not merely a matter of aesthetics. It is a matter of evoking the very soul of the place. Consider the furnishings, the light, and the spirit you wish to create within those walls. For even the smallest of choices can have a great impact on the journey that lies ahead.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAUSE** here. Did you do the task above? What did you learn?"
      ],
      "metadata": {
        "id": "coNygYQhQP9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![1F6D1_color.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmoAAAJqCAMAAACsDJxNAAABwlBMVEVHcEwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD///8AAADSLyf++/vaVlCoJh8qCQjYS0T21dThdG8CAQD//v6FHhkZBgXUNi///f0cBgW5KSKKHxq3KSICAADkgX3VPjfqn5vlh4LUODDVPDXzyMX0zMr32NbpmJT21NLpm5ftrKn99fXcW1XvtbLqnprTMSnTNCz++fj77ez+/PzrpKDSMCjwurjnkIzXR0DWQDnmi4broZ7sqKTvtrT65+b219XlhYDXQzz43NvaUkvhc23ZTkfdYlzaVU7vs7D10c/bWVL99PTnj4vusa7id3LjfHfolZHYSUL4397VPTX88O/fa2bcXVe4KSLywr/88vLxv7z44eDok47tqqfpmpb99/fyxML55OPdX1nzx8XTNi7spaL1z8376+reZV/ur6zieXT32tj109HfaGLpl5PVOjPqnZngb2n+/fznko3bV1Hjfnn0y8nzycfliYR8HBcRBAN5NSGyAAAAJHRSTlMA+thQ+EoZKlUW09b5pQ8hzt/vCE5sBCaVhjpbxbv9MkXB51FC1veIAAASb0lEQVR42uzd+1MTSQLA8X24pVuHerp75d5WrbdVGwbrkEot3NUV6CruETAJj/BIAvKQN0TIigsI4gMF9RQEBWv/39vaPW8h3UO6J8NMuvP9/qgJNdCf6p5JOpOPPiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIrKnL85+/d0pY/ru67NfMGYmdvYv33xcbVgX//zXvzFyhnXydLWJXayu/gZsJvX56YvVxnbx9BlG0JQufFJtdOdOMoZm9Nn5asM7/xmjaIS0E9XG9/GnjCPSsEY2ScNa+fdlVbUlVX3JaDKnMa+RPXMa8xpzGvMa2ScNa0jDGtKwRkjDmu3Xnr/83Zh+4TrU5Dnt+385xvTPfzCvIQ1rZJM0rCENa2SbNKwhDWtkmzSsIQ1rSLNNGtaQhjWk2SYNa0jDGtJsk4Y1pGENaUrSEkO1ZdZQAmvWSRtfmeyIRsquaMfkygjW7JGWyOUjZVw+l8CaHdI2b0TKvFubWLNBWrohUvZFX6ewZrq09f6IEfWvY81safXXI4bU/APWkIY1pNkkDWtGS3sSMaon9VhjTmNeQ5o9cxrzGnMa8xrS7JOGNaRhDWm2ScMa0rCGNDuuPbkOtWpOu1Ju27p/ZF5DGtaQZpU0rCENa0izTRrWkIY1pNkmDWtIwxrSbJOGNaRhDWm2ScMa0rCGNNukYQ1pWEOabdKwhjSsIc02aVhDGtaQZps0rCENa0izTRrWkIY1pNkmDWtIwxrSbJOGNaRhDWm2ScMa0rCGNNukYQ1pWEOabdKwhjSsIc02aVhDGtaQhjWsIQ1rSMMa0pCGNaRhDWlYwxrSsIY0rCENaVhDGtaQhjWsIQ1rSMMa0pCGNaRhDWlYwxrSsIY0rCENaVhDGtaQhjWsIQ1rSMMa0pCGNaRhDWlYq3hrSMMa0rCGNKRhDWlYQxrWKtka0rCGNKzZZA1pWEMa1myyhjSsIQ1rNllDGtaQhjWbrCENa0jDmk3WkIa1YPq2ykXaz0gL29rP38vHpupbE6VdOM+cZt68dv6CedLOfMKcZuK89qczxlH7ijnNzHntK9OknT2BNDOtVZ01jNppVk9T19DThp2pVSHNVGtVZp2tnWL1NHcNPWX8+ok0Q6yZtYKeY/U0dw09ZxS1E8xp5s5rJ0yS9jnSTLZm0nXBBeHoWT3Ldw0VBsukN6dOCkcv+RWTT1z+IA+wUmoPXP60T5KSBwuDddI2ajHmtODntdlKpJZGWhjW0pVHbWqH1TOMNbRzquKoXWdOC2de26s0aq+QFpa1zQqjts/qGdYaer2yqDV1MqeFNa/V/FRR1F4gLTxruYqiNsvqGd4aOltR1JqZ08Kb1/Yqitp9pIVnLV9R1KYLf/1tTBxX24V/66WKolZT+OtfgsRxdUm4BIUaQQ1qUIMaQQ1qUIMa1KBGUIMa1KBGUIMa1KAGNagR1KAGNagR1KAGNahBDWoENahBDWoENahBDWpQgxpBDWpQgxpBDWpQgxrUoEZQgxrUoEZQgxrUoAY1qBHUoAY1qDlO8m1dOhObeJ7ZHXzTq/icvneZ2KFmr9/IDzf3H/7HbHpE9RjG62bi2Vg2PlM3Xq919Km2mdWYS5mVuvEE1MqDWuJey2T7wR+2sbw7WvRZQ7OdEcWa24ofw1jLs4M/r3OyZUwZyMLLIgewsbqQhFro1LYa87LRuZzuO/Jp9+5G1Ou8evQxzP84IHnWwHat0pSWUTmE6Wwf1EKlNpptdRuc9kyT+/PGWyNaNR5xDLUxt/mxYXGq+K8QVzyE6bkfoBYatfX4zlGDs9Sz7rbeDetJi7S7TlDXnh51DNGJYpPRWFT5IDbeQC0kavfyxQbn/rj8mXcius26HENdR5En3s0d/Us80ziIhpUU1EKglsg0FB+ctTvS5+5rU1uTXlMmnyo8deKohW9U7zD666EWOLWth2qDsyq5dkusaVOLPJa9XHJZ6ak3htx/Dd359dk61AKmtnVZdXAmxEWnT19aRLIOzt9XfO4j99de5nSPYwJqwVKrb1YfnKzw7CYP1OpEaQPKT+5wtdaifSAzUAuUWr/O4OwKC+i0PrV7wtSY13j2QK9v1HYWoBYgtUGtwYkK51kPtQd4uvCULzmp9fxb635RizxKQi0war2aL8AOFw5Onfb4rhYeQ1epP8AztUgaaoFRW9YdnJ7CFfQ/upNa4UXkm2jpJ3teqS39BLWAqD3Wf1Ws8FSp9q7W8xsKndQ/0j6Gjmt+UYu0QC0gasv6gzNX+DNGdKysCa90XPEA5Klv1AZSUAuEWm2D/uDcFwZnK6N6wtfQLbxU0VvjAchOrV/UIlNQC4TajMub0Q+z8czzZhdAY5LX5hZ64h8SDmjiw/+8zkneMHd7P2pjcTW+urjh8r8xFWrD8UN17XWWuIJCzTs12asMratvP5zxb07UKA7zgQSgR+6G7JO+sdXa9eHN/ZGM9IW7ml4FajeFPZwTkktqqAVBLSmR1H1o5hndk6ygflJLS/d+HDyGppjsISteqDnOiviT+qAWALUp8SXawk2yiax4Zu8ntRuSDW2FVw51kpkv742a061yPgA136m9EP7u2+ImauEiddpHaiOStfGVuOdxTeV0XomauNPoHdQCoPZe2Hwo2cM136B3dqNFTbJ+yrbF5Yq/lKxIzRku4Q0DqHmmJmy7WVbZ3Jr1kZr4ul639HHi+fwTj9SEFfQ11MKgtqiw5TD61kdqwjsN7fJ9G0PCEtqa8EZtuehL0lA7BmpXhdMw2Ts+icOn7v2Of9T6lD94sCo8ct4btUcsoGFQE0+Bnks37S8dvPS75iO1MeEANt0+ZyM88pUnauLPeQG1AKiNi2fb27L3BMf/2CT77yHHR2rCxwHW3DaQJZYKHzrohVpC/FRVG9SCeAm3XbR2XbaheuvK76/Zb6SL39JAh5pwCXzZ9aHPiu7WVqCWfC6+LbsFtSDemNqTvSW++Ebyksf6q3TP4GOVe2foUNtVugSW70tv0aaWGpO8ETfJG1OBULsqfzO78+XybHZ3cGHEy317dKg9ED6c6fpQ4e2prqLUBroPtb+hstUTasdDbajIDp72ycxm8hipqV00eqWm1CjUgtkaeVNhT3Rm3mJqeXbhBkRtXGVf/87yprXU0lAL6mMss2ojsj9vJzW9T+dBrRRqfUtqY9J6NWEjtZwDtaCoqX+Os3nePmqXU1ALjprzVHVcOtpso9aq+beCWmnUEsof0FvbtIta9LYDtSCpOUnl22603rOK2pwDtWCpOckJ1cG522sRtYkU1IKm5qR6VD/3O5m0hVp0Tv9muFArmZrjtOV9W3TMoNZ628NfCWo+UHOSK2o3tV2bt4La5LgDtZCoOU7fitJdQvstoJa/nXKgFh61X2e2nMKd/zvnTad2a9Drt0xBzS9qv9bb2F1sbsuYTK1z/32v978O1Hyk9ttK+jg3sx3r33O5C1CxO5KVz9bIGz2HSje+GUmW9JeBWonUhnIzPXdky+LonFRbm3/UStnw/aCEKRJqYVC7tPj7jRKeyW6TkpyRXJfO+Ect6I+xQC1Eau////zoFdnKODKgew1a1h/Og1po1NJHnv38tk9XmNde+kct+I8cQy0kam8P37JT+iWZwm2RB/yjVsqNFHqhZhK1gj0dw7IlVLjffKt/1MTbw9Qc9+1hoBYKtd7CW6c9Vlm6/LxrZPA3vYJaKNSEu0bKvlLndeGDNnykJrmVn+wujj7eyg9qoVATbkJcI947rVa4w7afd42U3aBU/Do72Q1K30LNJGril7VujBSusXnFJc4bNeeWuJGspvAWpTkfb7sMtXCovZPs4xo8eLqduiP5AqldP6lJbyZ/8+Cdtfqeyx6yAjWjqNXKPtueT38Y6KbGYdkoj/lJTf4VGWvZqf9dDF/qkn4njLevyIBaeC92uGwaut+djWf6X+7IP6OX8JOa62cDOx7G4rGHJX3xD9TKiFqjhw+EZx1fqXn6OrPOWqgZRm1rTXuUoyP+UnPiHqh1OVAz7T3QmPYoLzs+U/Pw1bMb16BmHLWmDt2la9xvas6C9hdqyz/tBLXy3kT0TnOUrzi+U3My/pwtQq28qaX2tUZ5uP4YqCUn9W4ftA41I3fh9g5ojPK0wp1j9ak5Tfn/tnd3P1WbcQDHTxY1YcQJEoiKu9C0NYsDzEiWQAYRXyPqxKHyZlA0aHwZYFwccRpxW8yYu9iFf/B2sajsnAOt4aXP08/3vk2TfvJ7eKw9LXANL5p9OQG1sr9bcDv/n2sNHk9uCrUi3i8fz1AL9TWW8+fySsv14wN1cvO8SP4471x7cTv/E93fUSvby3kvf823ev6R62z//25x32Ceo67l+3ttep3vDtW9qLCKWuneA715Ms+OIOeJJ4r9n6P3e4OVHP/mcXe9XcntvO8poLZz1LLRG/0bLZ5n8r6tO1/sXb4P3dpouF5+UOiJ7tQoauWjlmWzT9e9y2/m859q7X/inhrKfeDQhe/XeyJ299oGx8+tnYsPM9TKSC3Llt40/4moB0UGxLWPt5PH5opcw/GVS00uof/q7MaHr3mcOjmGWkmp/bv0rSw2WjqvPil4nuMftpP3lgoeO77a6JHo1Jlcv2Q/9tFzh8mLGWqlpZZlr3omTq854bO7DwaLn2boxrn/RtHj4gePPVl9/fE1HHt96uVY0dH8YvNXT9Q2/ZeIXj15eP3UxOTZldW/fxv81JP82fP27JWBT/59qYuzt66/nTg7ceF+z51i02ns/P3l5Z/nRjPUyk9NqKGGGmqooSbUUEMNNaGGGmqooYaaUEMNNdSEGmqooYYaakINNdRQE2qooYYaaqgJNdRQQ02ooYYaaqihJtRQQw01oYYaaqihhppQQw011IQaaqiVhdo8ElvVfLWp1X38ZGSQia1p+HTdh0krRW0kYW2npCXTlaL2S8LaTklLnlaKWqPvtk4vkLHZLUzn+BBv3NTmGn1QwlzbjpmWJC8rRW10MTHXdmamJYujlaKWLSes7Yy0ZDmrFrWbzxNr6E6snsml8YpRa7gxYG3rpSUXsqpRG2zy4XNr6JaunsnUQuWoZXfOmWvbP9OeN3gCGD21rKfPXNvumdbXk1WRWjbQb65t70zr+y6Ljdq+uqs/wVpZpZ2ou1n7AqLWUXf1vayVVNq3vXX3qiMgarX9rAUrLd0fkrRaW8paGKtnvbS0LShqB1PWAp1paXowKGqHUtZClZYeCopa11HWwlw90/RoLaw6U9bCnGlpZ2DUutpZC1Nae0dg1GrdKWsBrp5p2l0LrgOsBTjT0r3hSat1HWEtPGmHOwKkVmtpZS201bO1pRZkLZ+zFtZM+2xPrcaaSGONNNZIq4401khjjbTYpLFGGmukxSaNNdJYIy02aayRxhppsUljjTTWSKvVWGONNNZIY4000lgjjTXSWGONNNZIY4000lgjjTXSWGONNNZIY4000lgjjTXSWGONNNZIY4000lgjjTXSWGONNNZIY4000lgjjTXSWKu6NdJYI4010khjjTTWSGOtytZIY4001mKyRhprpLEWkzXSWCONtZiskcYaaazFZI001khjLSZrpLFGGmsxWSONNdJYi8kaaayRxlpM1khjjTTWYrJGGmuksRaTNdJYI421mKyRxhpprMVkjTTWSGMtJmuksUYaazFZI4010liLyRppoVub6Wt8B6d7StZ0E2kzpIU+18LITGONNNZiskYaa6SxFpM10mLah5Zamr2nuWamsRaRNdKsoVZP1mKaa2Yaa6SxlmsNDcRav9UzeGtLP4Ug7d5fpIVvbfZZ+aX9OEtaDNYuDlwuN7TFH74hLQ5r2fCjkfJCG3k0nJEWjbUsG5+5Mnny65J1cvLKwHiTCyYtWGuBRRprpLEWkzXSWCONtZiskcYaaazFZI001khjLSZrpLFGmmKyRhprpCkma6SxRpreW2ttYu3dV8H0rom01hb3N4S5Fn5mGmuksUaaWCONNdK0GfvQULP3NNfMNMVkjTRrqNVTMc01M4010hTTGmr1DKMvvgxd2hHSAqnrYNjSDnS5h8G0uy1caG273b+g6m7rDdFZb1u3exdcezr3Hm7fFY6yXe2H93baeEqSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSJEmSVLB/ALpcI0MgX2CqAAAAAElFTkSuQmCC)"
      ],
      "metadata": {
        "id": "S0LX5_n5QRrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STOP** here - end of Lab 1-A."
      ],
      "metadata": {
        "id": "Ul_wjPzLRKs-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAB 1-B Complex Prompting"
      ],
      "metadata": {
        "id": "UFnuIeyhRMS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates\n",
        "\n",
        "Our prompts are starting to get a little complicated, with different types of messages and enforced order.\n",
        "\n",
        "Let's explore the use of a \"**prompt template**\" to help with all of this, giving us a structured starting point for our prompts to pass to the chat *LLM*."
      ],
      "metadata": {
        "id": "bPDELXC4RSU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict\n",
        "import pydantic\n",
        "\n",
        "class SimplePromptTemplate(pydantic.BaseModel):\n",
        "    city: str\n",
        "\n",
        "    def format_string(self) -> str:\n",
        "        return f\"What color paint should I use for a house in {self.city}.\"\n"
      ],
      "metadata": {
        "id": "RLP_27vcR55S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide the value for the placeholder to create the prompt\n",
        "City = \"San Diego\" #@param {\"type\": \"string\"}\n",
        "filled_prompt = SimplePromptTemplate(city=City.strip()).format_string()\n",
        "print(filled_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta6axu_nRK9c",
        "outputId": "c72ee909-eba9-43a1-c2ba-7f54c05568d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What color paint should I use for a house in San Diego.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system message\n",
        "SYSTEM_MESSAGE = 'You are a helpful assistant who understands the business of interior decorating.'\n",
        "\n",
        "messages_chat = [\n",
        "    {\"content\": SYSTEM_MESSAGE, \"role\": \"system\"},\n",
        "    {\"content\": filled_prompt, \"role\": \"user\"}\n",
        "]\n",
        "\n",
        "### Note: We can add streaming in order to help with seeing the initial outputs sooner.\n",
        "response = completion(model=llm_model, messages=messages_chat, temperature=1)\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UCgfMuGtSLer",
        "outputId": "2fb4161d-d0ce-4df4-aafc-4a88066d0dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, let's talk paint colors for a house in San Diego!  Here's a breakdown to help you choose the perfect palette, considering the local environment and style:\n\n**Key Considerations for San Diego:**\n\n*   **Sunlight:** San Diego gets a lot of sunshine! This means colors will appear brighter and lighter than they would in a less sunny area. Consider how the light hits your house at different times of day.\n*   **Coastal Influence:** Many San Diego homes embrace a coastal vibe. This often translates to lighter, airier colors that evoke the ocean and sky.\n*   **Architectural Style:**  The style of your house matters.  Is it a Craftsman bungalow, a Spanish Colonial, a modern beach house, or something else? The paint color should complement the architecture.\n*   **HOA Restrictions:** If you're in a neighborhood with a Homeowners Association (HOA), check their guidelines before committing to a color.\n*   **Personal Preference:** Ultimately, you need to love the colors you choose!\n\n**Popular Color Palettes for San Diego Homes:**\n\n**1. Coastal/Beach-Inspired:**\n\n*   **Whites and Off-Whites:**  Classic, clean, and reflect light beautifully.\n    *   Examples: Benjamin Moore White Dove, Sherwin-Williams Alabaster, Dunn-Edwards Swiss Coffee.\n    *   **Why:** Versatile, creates a bright and airy feel, works well with any accent color.\n*   **Light Blues and Greens:** Evoke the ocean and sky.\n    *   Examples: Sherwin-Williams Sea Salt, Benjamin Moore Palladian Blue, Dunn-Edwards Crystal Springs.\n    *   **Why:** Relaxing, refreshing, complements natural landscapes.\n*   **Sandy Beiges and Taupes:**  Warm and natural, reminiscent of the beach.\n    *   Examples: Sherwin-Williams Accessible Beige, Benjamin Moore Edgecomb Gray, Dunn-Edwards Warm White.\n    *   **Why:** Earthy, grounding, creates a cozy feel.\n*   **Accents:** Consider pops of coral, navy blue, turquoise, or yellow for doors, shutters, or trim.\n\n**2. Earthy/Mediterranean:**\n\n*   **Warm Whites and Creams:** Soft and inviting.\n    *   Examples: Benjamin Moore Navajo White, Sherwin-Williams Creamy, Dunn-Edwards Bone White.\n    *   **Why:** Creates a warm and welcoming atmosphere, complements terracotta tiles and stucco.\n*   **Terracotta and Clay Tones:**  Rich and grounding.\n    *   Examples: Sherwin-Williams Baked Clay, Benjamin Moore Terra Cotta Tile, Dunn-Edwards Adobe Villa.\n    *   **Why:** Adds warmth and depth, evokes a Spanish Colonial or Mediterranean feel.\n*   **Olive Greens:**  Natural and calming.\n    *   Examples: Sherwin-Williams Dried Thyme, Benjamin Moore Olive Branch, Dunn-Edwards Olive Tree.\n    *   **Why:** Complements terracotta and warm whites, adds a touch of nature.\n*   **Accents:**  Deep blues, wrought iron black, or vibrant reds for a pop of color.\n\n**3. Modern/Contemporary:**\n\n*   **Cool Grays:**  Sophisticated and versatile.\n    *   Examples: Sherwin-Williams Repose Gray, Benjamin Moore Gray Owl, Dunn-Edwards Foggy Day.\n    *   **Why:** Creates a clean and modern look, works well with bold accents.\n*   **Charcoal Grays and Blacks:**  Dramatic and stylish.\n    *   Examples: Sherwin-Williams Iron Ore, Benjamin Moore Kendall Charcoal, Dunn-Edwards Black Pool.\n    *   **Why:** Adds contrast and depth, perfect for accent walls or modern exteriors.\n*   **Bright Whites:**  Minimalist and clean.\n    *   Examples: Sherwin-Williams Pure White, Benjamin Moore Chantilly Lace, Dunn-Edwards Cool December.\n    *   **Why:** Creates a bright and airy feel, emphasizes architectural details.\n*   **Accents:**  Bold colors like orange, yellow, or teal for doors or architectural features.\n\n**Tips for Choosing Paint Colors:**\n\n*   **Get Samples:** Always, always, always get paint samples and test them on your walls. Observe how the colors look at different times of day and in different lighting conditions.\n*   **Consider Undertones:**  Pay attention to the undertones of the paint.  Gray can have blue, green, or purple undertones, and beige can have pink or yellow undertones.  Choose undertones that complement your existing decor and the natural light in your home.\n*   **Look at the Big Picture:**  Consider the colors of your roof, landscaping, and any existing architectural elements.  Choose paint colors that coordinate with these elements.\n*   **Drive Around:**  Take a drive around your neighborhood and see what colors you like on other houses. This can give you inspiration and help you narrow down your choices.\n*   **Consult a Professional:** If you're feeling overwhelmed, consider consulting with an interior designer or color consultant. They can help you choose the perfect paint colors for your home and lifestyle.\n\n**To give you more specific recommendations, tell me a little more about your house:**\n\n*   What style is it (e.g., Craftsman, Spanish Colonial, modern, ranch)?\n*   What colors are your roof and trim?\n*   What is your personal style preference (e.g., coastal, modern, traditional)?\n*   Are you painting the interior or exterior of your house?\n*   Do you have any furniture or decor that you want to coordinate with?\n\nOnce I have a better understanding of your needs, I can give you more tailored recommendations. Good luck!\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token Mechanics\n",
        "\n",
        "Our goals in this section are to understand what tokens are, how they are used in LLMs, and how to estimate both input and output costs of token use. Tokens can be thought of as pieces of words. Before the LLM API processes a request, the input is broken down into *tokens*. These tokens are not necessarily defined by exactly where the words start or end - tokens can include trailing spaces and even sub-words.\n",
        "\n",
        "* For hands-on exploration of tokenization, you can use OpenAI's [interactive Tokenizer tool](https://platform.openai.com/tokenizer), which allows you to calculate the number of tokens and see how text is broken into tokens.\n",
        "\n",
        "* You can find details on OpenAI's token pricing [here](https://openai.com/pricing).\n",
        "\n",
        "Let's first look at an extremely simple example of tracking token usage for a single LLM call."
      ],
      "metadata": {
        "id": "hyeh5XwESdFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USER_INPUT = 'Can you tell me a 5-sentence joke?'\n",
        "response = completion(model=llm_model,temperature=0.8,\n",
        "                      messages=[{'role': 'user', 'content': USER_INPUT}])\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n",
        "\n",
        "print(f\"Usage Information:\\n{response.json()['usage']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "4z27T5vLSdYZ",
        "outputId": "6ecf8595-0102-4532-d38b-2cb97fafbd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why don't scientists trust atoms? Because they make up everything! I tried explaining it to my friend, but he didn't get it. I said, \"No, you don't understand, they're always fabricating the truth!\" He just rolled his eyes. Guess you could say the joke... lacked chemistry.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage Information:\n",
            "{'completion_tokens': 68, 'prompt_tokens': 11, 'total_tokens': 79, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': None, 'text_tokens': 11, 'image_tokens': None}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3\n",
        "\n",
        "Notice that if you rerun the above cell multiple times, you will get different prompt outputs and costs (total tokens) associated with it!\n",
        "\n",
        "Try running that code a few more times.\n",
        "* Does the output change?\n",
        "* Does the output change if you change the prompt?\n",
        "* Does the output change if you make the prompt more complex?"
      ],
      "metadata": {
        "id": "cwiuw50oSwou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### For this example, we will set the llm_model to gemini-2.5-flash\n",
        "llm_model = \"gemini/gemini-2.5-flash\"\n",
        "USER_INPUT = 'Can you tell me a 5-sentence joke?'\n",
        "\n",
        "response = completion(model=llm_model,temperature=0.3,\n",
        "                      messages=[{'role': 'user', 'content': USER_INPUT}])\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))\n",
        "\n",
        "print(f\"Usage Information:\\n{response.json()['usage']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "GOG54ggsS0YN",
        "outputId": "1d2dc2b2-ac46-49d1-8de9-87fc3d30d124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's a five-sentence joke for you:\n\n1.  A man walks into a bar with his dog and orders a drink.\n2.  The bartender, curious, asks, \"Does your dog bite?\"\n3.  The man confidently replies, \"No, my dog doesn't bite.\"\n4.  The bartender reaches down to pet the dog, and it promptly bites his hand.\n5.  \"Hey, I thought you said your dog doesn't bite!\" the bartender exclaims, to which the man calmly replies, \"That's not my dog.\""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage Information:\n",
            "{'completion_tokens': 929, 'prompt_tokens': 12, 'total_tokens': 941, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 806, 'rejected_prediction_tokens': None, 'text_tokens': 123}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': None, 'text_tokens': 12, 'image_tokens': None}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAUSE** here. Did you do the task above? What did you learn?"
      ],
      "metadata": {
        "id": "waThTSruX8Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pause pillow.jpeg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "LB8p5vF2X9W1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Experimentation\n",
        "\n",
        "For the second part of our lab, we will perform:\n",
        "\n",
        "- **Model Comparison**: Evaluate different LLMs\n",
        "- **Parameter Experimentation**: Experiment with temperature, top_p and other parameter settings\n",
        "- **Advanced Techniques**: Practice zero-shot, few-shot, and Chain-of-Thought prompting"
      ],
      "metadata": {
        "id": "PrM8-80ZYFl5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Comparison**\n",
        "\n",
        "Let's start by comparing Gemini's `gemini-2.0-flash` with Gemini's `gemini-2.5-flash`.\n",
        "\n",
        "Check to see if the responses differ at all from the previous responses you received."
      ],
      "metadata": {
        "id": "JJ0uzxliYKwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_models_df['Model Name'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC38o7NnuW6R",
        "outputId": "4c446b59-006d-4044-d9aa-7411dc528d25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['gemini/gemini-1.5-flash', 'gemini/gemini-1.5-flash-002',\n",
              "       'gemini/gemini-1.5-flash-latest', 'gemini/gemini-2.0-flash',\n",
              "       'gemini/gemini-2.0-flash-001', 'gemini/gemini-2.0-flash-lite',\n",
              "       'gemini/gemini-2.0-flash-lite-preview-02-05',\n",
              "       'gemini/gemini-2.5-flash', 'gpt-4.1-mini',\n",
              "       'gpt-4.1-mini-2025-04-14', 'gpt-4.1-nano',\n",
              "       'gpt-4.1-nano-2025-04-14', 'gpt-5-mini', 'gpt-5-mini-2025-08-07',\n",
              "       'gpt-5-nano', 'gpt-5-nano-2025-08-07'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_to_test = \"What is the biggest animal in the world?\"\n",
        "\n",
        "\n",
        "\n",
        "gemini_2_0_flash_response = completion(model='gemini/gemini-2.0-flash',temperature=0.8,\n",
        "                      messages=[{'role': 'user', 'content': prompt_to_test}])\n",
        "\n",
        "gemini_2_5_flash_response = completion(model='gemini/gemini-2.5-flash',temperature=0.8,\n",
        "                      messages=[{'role': 'user', 'content': prompt_to_test}])\n",
        "\n",
        "print(\"Gemini-2.0-flash response:\")\n",
        "display(Markdown(gemini_2_0_flash_response.choices[0].message.content))\n",
        "print(\"Gemini-2.5-flash response:\")\n",
        "display(Markdown(gemini_2_5_flash_response.choices[0].message.content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "9xYmSlgLXz_Z",
        "outputId": "8ef6fab1-0a43-4d9a-e7b2-4158608c3ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini-2.0-flash response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The biggest animal in the world is the **blue whale** ( *Balaenoptera musculus*).\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini-2.5-flash response:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The biggest animal in the world is the **blue whale**.\n\nIt is the largest animal known to have ever lived, by both length and weight.\n\n*   **Length:** Up to 30 meters (100 feet) â€“ roughly the length of three school buses.\n*   **Weight:** Up to 200 tons (400,000 pounds) â€“ equivalent to about 30 elephants."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAUSE**, and consider the above. Play with it! Try other questions!"
      ],
      "metadata": {
        "id": "nnNwN5S8YrsN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pause pillow.jpeg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "KkF1IRmvYszv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parameter Experimentation\n",
        "\n",
        "Try simple things like changing the temperature of the model."
      ],
      "metadata": {
        "id": "tkymWMyjYvlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for temperature in [0, 0.5, 1]:\n",
        "  print(f\"Temperature = {temperature}:\")\n",
        "  response = completion(model='gemini/gemini-2.0-flash',temperature=temperature,\n",
        "                      messages=[{'role': 'user', 'content': \"Why is the sky blue? Answer in 30 words or less\"}])\n",
        "  display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "W0DEaZSwX2X6",
        "outputId": "3d162c08-e313-4457-d007-bc95c0b04fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature = 0:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky is blue because of Rayleigh scattering. Air molecules scatter blue light from the sun more than other colors, making the sky appear blue in all directions.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature = 0.5:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky is blue because of Rayleigh scattering. Blue light from the sun is scattered more by the Earth's atmosphere than other colors, making the sky appear blue.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Temperature = 1:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The sky is blue because of a phenomenon called Rayleigh scattering. Air molecules scatter blue light from the sun more than other colors, making the sky appear blue.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![pause pillow.jpeg](data:image/jpeg;base64,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)"
      ],
      "metadata": {
        "id": "d6BnueMfZPwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PAUSE** here. What did you notice about the different temperature settings?"
      ],
      "metadata": {
        "id": "moVvGezYZSBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Techniques\n",
        "\n",
        "Practice zero-shot, few-shot, and Chain-of-Thought prompting."
      ],
      "metadata": {
        "id": "A71v9XeVZWNI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zero-Shot Prompting**\n",
        "\n",
        "Zero-shot prompting occurs when a model is asked to perform a task without any prior examples or context about how to perform that specific task. The prompt must be self-contained, as the model relies solely on its pre-trained knowledge to generate a response.\n",
        "\n",
        "Example Prompt:\n",
        "* Summarize the key impacts of the 2020 global pandemic on remote work trends.\n",
        "\n",
        "In this case, the model is expected to understand what is being asked and use its internal knowledge to provide a summary without any additional examples or guidance.\n"
      ],
      "metadata": {
        "id": "J4joKCx5ZXBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try zero-shot\n",
        "prompt = \"Summarize the top 3 key impacts of the 2020 global pandemic on remote work trends.\"\n",
        "res_zero = completion(model='gemini/gemini-2.5-flash',temperature=1,\n",
        "                      messages=[{'role': 'user', 'content': prompt}])\n",
        "\n",
        "display(Markdown(res_zero.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "aeUDWiNaZP7g",
        "outputId": "2a3c5427-bbaa-4e59-f399-289f0ff38d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The 2020 global pandemic profoundly reshaped remote work trends, leading to three key impacts:\n\n1.  **Rapid, Widespread Adoption and Normalization:** Remote work shifted almost overnight from a niche benefit or occasional flexibility to a mandatory, mainstream mode of operation for countless businesses. This forced immediate adoption, proving its viability on a massive scale and fundamentally changing perceptions of its effectiveness.\n2.  **Significant Investment in Enabling Technology and Policy Frameworks:** To support the sudden shift, companies poured resources into collaborative software, secure remote access, cloud infrastructure, and video conferencing tools. Simultaneously, new HR policies, management strategies, and communication protocols were developed to manage distributed teams effectively.\n3.  **Emergence of Hybrid Models and Expanded Talent Pools:** While not all companies remained fully remote, the pandemic cemented the viability of flexible work. This led to a widespread adoption of \"hybrid\" models, combining in-office and remote days. Crucially, it also dissolved geographic barriers for many roles, allowing companies to tap into a much broader, global talent pool and employees to seek opportunities beyond their local area."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Few-Shot Prompting**\n",
        "\n",
        "Few-shot prompting involves providing a few examples to the model before presenting the actual task. This approach helps the model understand the desired format or the specific type of response needed.\n",
        "\n",
        "Example Prompt:\n",
        "\n",
        "```\n",
        "Example 1: Translate the English phrase \"Hello, how are you?\" to French.\n",
        "Answer: Bonjour, comment Ã§a va?\n",
        "\n",
        "Example 2: Translate the English phrase \"What time is it?\" to French.\n",
        "Answer: Quelle heure est-il?\n",
        "\n",
        "Prompt: Translate the English phrase \"Where is the nearest restaurant?\" to French.\n",
        "```\n",
        "\n",
        "Here, the model is shown two examples of translation tasks, which sets the expectation that it should translate the third phrase from English to French in a similar format."
      ],
      "metadata": {
        "id": "8Un5CxRAZh22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try few-shot prompting\n",
        "prompt = \"\"\"\n",
        "  Below are a few examples, followed by the prompt for you to follow.\n",
        "\n",
        "  Example 1: Translate the English phrase \"Hello, how are you?\" to French.\n",
        "  Answer: Bonjour, comment Ã§a va?\n",
        "\n",
        "  Example 2: Translate the English phrase \"What time is it?\" to French.\n",
        "  Answer: Quelle heure est-il?\n",
        "\n",
        "  Prompt: Translate the English phrase \"Where is the nearest restaurant?\" to French.\"\"\"\n",
        "\n",
        "res_few = completion(model='gemini/gemini-2.5-flash',temperature=1,\n",
        "                    messages=[{'role': 'user', 'content': prompt}])\n",
        "\n",
        "display(Markdown(res_few.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "0UqK2FTFZgOv",
        "outputId": "99e43280-e64c-4d46-f71e-577ae8fa89db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Answer: OÃ¹ est le restaurant le plus proche?"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chain-of-Thought Prompting**\n",
        "\n",
        "Chain-of-Thought prompting guides the model through a logical sequence of steps or thoughts to reach a conclusion, particularly useful for complex reasoning tasks. This method helps models handle problems that require multiple inferential steps.\n",
        "\n",
        "Example Prompt:\n",
        "\n",
        "```\n",
        "Question: A farmer has 17 sheep, and all but 9 die. How many are left alive?\n",
        "Chain of Thought: The phrase \"all but 9 die\" means that 9 sheep remain alive.\n",
        "Answer: 9\n",
        "```\n",
        "\n",
        "In this example, the prompt explicitly outlines the reasoning process needed to answer the question, leading the model to the correct interpretation and response."
      ],
      "metadata": {
        "id": "T3nJotrUZouv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try chain-of-thought prompting\n",
        "prompt = \"\"\"\n",
        "  I will pose a question, and you will provide your chain-of-thought reasoning to reach the answer.\n",
        "\n",
        "  Question: A farmer has 17 sheep, and all but 9 die. How many are left alive?\"\"\"\n",
        "\n",
        "res_cot = completion(model='gemini/gemini-2.0-flash-lite',temperature=1,\n",
        "                    messages=[{'role': 'user', 'content': prompt}])\n",
        "\n",
        "display(Markdown(res_cot.choices[0].message.content))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "2lB9VyqQZo6f",
        "outputId": "7626db2d-3fb3-4593-be49-b76ab3583b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's my chain of thought:\n\n1. **The problem states \"all but 9 die.\"** This phrase indicates that only 9 sheep survived the event.\n2. **The question asks how many are left alive.**\n3. **Therefore, the answer is 9.**\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Usage Tips:**\n",
        "\n",
        "* **Zero-Shot**: Useful when querying general knowledge or opinions, or when the task is straightforward.\n",
        "* **Few-Shot**: Effective for tasks that require specific formats or where a model needs examples to \"learn\" the task quickly.\n",
        "* **Chain-of-Thought**: Best for complex reasoning, mathematics, or multi-step logical problems."
      ],
      "metadata": {
        "id": "NXTPyCZ-Zu6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. AI Safety Challenge\n",
        "\n",
        "For the third and final part of this lab, we will do a:\n",
        "\n",
        "- **Prompt Hacking Exercise**: Test the limits of AI safety with the Gandalf challenge*\n",
        "  - URL: https://gandalf.lakera.ai/baseline\n",
        "  - Objective: Attempt to \"hack\" the AI chatbot using creative prompting techniques\n",
        "  - Time: Utilize any remaining lab time to see if you can get through all of the levels!\n",
        "\n",
        "*Note: For further exploration, look into safety tools like Meta's Prompt Guard and LLaMA Guard.\n"
      ],
      "metadata": {
        "id": "P4f24XD9Zv_w"
      }
    }
  ]
}